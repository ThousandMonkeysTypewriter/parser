Как работает?

1. Запускать из директории parse_js/parse_js/spiders команду:

`scrapy crawl pages -a query=<запрос> -a output_filename=<имя_файла>`

По запросу <запрос> будут загружены все страницы поисковой выдачи сайта (https://searchcode.com/) и записаны в файл <имя_файла>.

2. Запускать далее:

`scrapy crawl code -a input_filename=<имя_файла> -a output_directory=<имя_директории>`

Переходит по каждой ссылке в файле <имя_файла>, и по каждому скрипту из поисковой выдачи. Заходит в raw view каждого скрипта и записывает
все скрипты в директорию <имя_директории>.
